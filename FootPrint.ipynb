{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgJRuatrkXmBPVlx690I1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhyutiLABS/KnowYourFootPrint/blob/main/FootPrint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carbon footprint of simple pytorch models**\n",
        "\n",
        "\n",
        "**Models considered**\n",
        "*   MLP [keeping the trainable parameters the same]\n",
        " - Shallow but wide [100 x 2]\n",
        " - Moderate [70 x 4]\n",
        " - Deep but narrow [40 x 8]\n",
        "\n",
        "**Objective:** is to determine which network has a higher carbon footprint given the same order of trainable parameters.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "*   Use pytorch to construct the MLP models\n",
        "*   Use codecarbon's `EmissionsTracker` to determine the emissions for each model built\n",
        "\n"
      ],
      "metadata": {
        "id": "A5KsVUhRh7S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon"
      ],
      "metadata": {
        "id": "wMBU0f9mkt-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrKyZDyphmY0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Reference for the MLP code: https://github.com/GLAZERadr/Multi-Layer-Perceptron-Pytorch/tree/main\n",
        "'''\n",
        "\n",
        "#Import all dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset/clean.csv')\n",
        "df.head()\n",
        "\n",
        "X = df.drop('gdp', axis=1).values\n",
        "y = df['gdp'].values\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "class factbook_data:\n",
        "    def __init__(self, X, y, scale_data=True):\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            if scale_data:\n",
        "                X = StandardScaler().fit_transform(X)\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "PuauWRlskBlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP module\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers, hidden_neurons):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(5, hidden_neurons),\n",
        "            nn.ReLU())\n",
        "\n",
        "        for i in range(hidden_layers-1):\n",
        "            self.layers.append(nn.Linear(hidden_neurons, hidden_neurons))\n",
        "            self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.layers.append(nn.Linear(hidden_neurons, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def model_trainer(model, dataloader, loss_function, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "        current_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.float(), targets.float()\n",
        "            targets = targets.reshape((targets.shape[0], 1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = loss_function(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            current_loss = loss.item()\n",
        "\n",
        "            if i%10 == 0:\n",
        "                print(f'Loss after mini-batch %5d: %.6f'%(i+1, current_loss))\n",
        "                current_loss = 0.0\n",
        "\n",
        "        print(f'Epoch {epoch+1} finished')\n",
        "\n",
        "    print(\"Training has completed\")"
      ],
      "metadata": {
        "id": "bPeEm2jhnhfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = factbook_data(X, y, scale_data=False)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "G-8h4ZWWqrNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(trainloader.dataset))"
      ],
      "metadata": {
        "id": "4HYKjvhPs885"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_shallow = MLP(input_size = 5, output_size = 1, hidden_layers = 8, hidden_neurons = 40 )\n",
        "mlp_mid = MLP(input_size = 5, output_size = 1, hidden_layers = 4, hidden_neurons = 70 )\n",
        "mlp_deep = MLP(input_size = 5, output_size = 1, hidden_layers = 8, hidden_neurons = 40 )\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "def configure_optimizer(model):\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "2E8Poc1zqvBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "try:\n",
        "    # Compute intensive code goes here\n",
        "    optimizer = configure_optimizer(mlp_shallow)\n",
        "    model_trainer(mlp_shallow, trainloader, loss_function, optimizer, 100)\n",
        "\n",
        "finally:\n",
        "     tracker.stop()"
      ],
      "metadata": {
        "id": "Z2ylTq_4l_Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4OBFLhKdrfGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}